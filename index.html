<!DOCTYPE html>
<html>

  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <meta name="description" content="Portfolio : Work samples">

    <link rel="stylesheet" type="text/css" media="screen" href="stylesheets/stylesheet.css">

    <title>Portfolio</title>
  </head>

  <body>

    <!-- HEADER -->
    <div id="header_wrap" class="outer">
        <header class="inner">
          <h1 id="project_title">Marty Kube's Portfolio</h1>
          <h2 id="project_tagline">A collection of work samples</h2>
        </header>
    </div>

    <!-- MAIN CONTENT -->
    <div id="main_content_wrap" class="outer">
      <section id="main_content" class="inner">
	

	<h3>
	  <a href="https://github.com/martykube/kite-detector">
	    Image Classifier for Kitesurfing
	  </a>
	</h3>
	<p>There are a couple of products such as the <a href="https://shop.soloshot.com/">Soloshot</a> which is a camera and gimball for taking selfies while engaged in outdoor sports.  A similar feature is <a href="http://www.dji.com/intelligent-flight-modes#gsSmart">follow me mode</a> on a drone.  I wanted to find out what would be required to build an image classifier for Kitesurfing that could form the basis of a tracker.</p>

	<h3><a href="https://github.com/martykube/cs231n/blob/master/assignment2/ConvolutionalNetworks.ipynb">Convolutional Neural Network for classifiying the CIFAR-10 images</a></h3>
	<p>I'm auditing a Standford University class, <a href="http://cs231n.stanford.edu/">cs231n</a>, on Convolutional Neural Networks for Visual Recognition.  The first half of the class builds up to training a convolution neural network on the <a href="https://www.cs.toronto.edu/~kriz/cifar.html">CIFAR-10</a> dataset.  My results on the CIFAR-10 dataset are reported.</p>

	<h3>
	  <a href="http://devblog.mapquest.com/author/marty-kube/">Blogging for MapQuest</a>
	</h3>
	<p>As a paid blogger for MapQuest I wrote a series of post illustrating how to use the MapQuest Javascript API for annotating maps.</p>


	<h3>
	  <a href="http://nbviewer.jupyter.org/urls/dl.dropbox.com/s/h7ui6d2ky9hsg68/task-1-vision.ipynb">
	    Space Robotics Challenge Vision Pipepline
	  </a>
	</h3>
	<p>As part of a team competing in the <a href="https://www.nasa.gov/directorates/spacetech/centennial_challenges/space_robotics/about.html">Space Robotic Challenege</a> I wrote an iPython notebook for the computer vision portion of Task #1.  In Task #1 stereo cameras are used to locate a sequence of flashing LED's.  The notebook shows how to locate the LED in images from a ROS Gazebo Simulation.  The next steps are to implement the transformations from  image coordinate to real world coordinates.</p>

	
      </section>
    </div>

    <!-- FOOTER  -->
    <div id="footer_wrap" class="outer">
      <footer class="inner">
        <p class="copyright">Portfolio by <a href="https://github.com/martykube">martykube</a></p>
        <p>Published with <a href="https://pages.github.com">GitHub Pages</a></p>
      </footer>
    </div>

    

  </body>
</html>
